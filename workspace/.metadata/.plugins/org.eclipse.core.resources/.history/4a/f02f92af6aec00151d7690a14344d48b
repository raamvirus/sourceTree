/*save and load */
package com.spark.pkg

import org.apache.spark.SparkContext;
import org.apache.spark.sql.SQLContext;
import org.apache.spark.sql.api.java.JavaSQLContext;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.types.{StructType,StructField,StringType};
import  org.apache.spark.sql.types.IntegerType;
import org.apache.spark.sql.types._;


object SQLMain4 extends App{
  val sc = new SparkContext("local","app")
  val sqlContext = new SQLContext(sc)
  val path = /home/edureka/scalaspace/Module0/Working_Files/Chapter_5/data_titanic.json
  df.printSchema()
//--------------------------------------------------------------//
df.registerTempTable("authors")
val authorDF = sqlContext.sql(
  """
     |SELECT * FROM authors WHERE value >0
  """.stripMargin)
//authorDF.save("authors.parquet")//default is parquet // nullable is true
authorDF.save("authorsjson.json","json") //customrised store
  //authorDf.saveAsParquetFile()
 df.rdd.map(row => row.getInt(1))
//--------------------------------------------------------------//
println("loading.........")
val dfload = sqlContext.load("authors.parquet")
dfload.printSchema()
//--------------------------------------------------------------//
}

