/*apply schema to a specific RDD */
package com.spark.pkg

import org.apache.spark.SparkContext;
import org.apache.spark.sql.SQLContext;
import org.apache.spark.sql.api.java.JavaSQLContext;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.types.{StructType,StructField,StringType};
import  org.apache.spark.sql.types.IntegerType;
import org.apache.spark.sql.types._;


object SQLMain3 extends App{
  val sc = new SparkContext("local","app")
  val sqlContext = new SQLContext(sc)
  val rdd  = sc.parallelize(List(("raam",32),("Me",1)))  //val rdd :RDD[Row]
      .map {case(name,count) => Row(name,count) }

  val schema = StructType(List(
      StructField("name",StringType,nullable=false),
      StructField("value",IntegerType,false)
      ))
val df = sqlContext.createDataFrame(rdd,schema)
df.printSchema()
//--------------------------------------------------------------//
df.registerTempTable("authors")
val authorDF = sqlContext.sql(
  """
     |SELECT * FROM authors WHERE value >0
  """.stripMargin)
authorDF.save("authors.parquet")//default is parquet // nullable is true
authorDF.save("authorsjson.json","json") //customrised store
//--------------------------------------------------------------//

val dfload = sqlContext.load("authors.parquet")
}

